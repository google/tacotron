<html>
  <head>
    <meta charset="UTF-8">
    <title>Audio samples from "Parallel Tacotron: Non-Autoregressive and Controllable TTS"</title>
    <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
    <link rel="shortcut icon" href="../../images/taco.png">
  </head>
  <body>
    <div>
      <article>
        <header>
          <h1>Audio samples from "Parallel Tacotron: Non-Autoregressive and Controllable TTS"</h1>
        </header>
      </article>

      <p><b>Paper: </b><a href="#"><del>arXiv</del></a></p>
      <p><b>Authors: </b>Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron J. Weiss, Yonghui Wu.</p>

      <p><b>Abstract:</b>
        Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still a room for improvements in its efficiency during inference.
This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder.
        This model, called <it>Parallel Tacotron</it>, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware.
The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem.
To further improve the naturalness, we introduce an iterative spectrogram loss, which is inspired by iterative refinement, and lightweight convolution, which can efficiently capture local contexts. 
Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective naturalness with significantly decreased inference time.
 </p>

      <p><a href="../../index.html">Click here for more from the Tacotron team.</a></p>

      <h2>Comparison among systems: Evaluation set 1</h2>
      <p><i>Random sample from Evaluation set 1 in the paper.</i></p>
      <table>
        <thead>
          <tr>
            <th>NAT (Gaussian upsampling)</th><th>NAT (vanilla upsampling)</th><th>NAT (semi-supervised)</th><th>NAT (unsupervised)</th><th>Tacotron 2 (GMMA)</th><th>Tacotron 2 (LSA)</th>
          </tr>
        </thead>
        <tbody>
          <tr><td colspan="6"><span>1: Take the next left onto 42nd Avenue South.</span></td></tr>
          <tr>
            <td><audio controls=""><source src="comparison/nat_gaussian/101.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="comparison/nat_vanilla/101.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="comparison/nat_semi/101.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="comparison/nat_unsup/101.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="comparison/tacotron_gmm/101.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="comparison/tacotron_lsa/101.wav" type="audio/wav"></audio></td>
          </tr>
        </tbody>
      </table>


      <h2>Comparison among systems: Evaluation set 2</h2>
      <p><i>Random sample from Evaluation set 2 in the paper. Comparison with real human speech.</i></p>

    </div>
  </body>
</html>
