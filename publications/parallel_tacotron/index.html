<html>
  <head>
    <meta charset="UTF-8">
    <title>Audio samples from "Parallel Tacotron: Non-Autoregressive and Controllable TTS"</title>
    <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
    <link rel="shortcut icon" href="../../images/taco.png">
  </head>
  <body>
    <div>
      <article>
        <header>
          <h1>Audio samples from "Parallel Tacotron: Non-Autoregressive and Controllable TTS"</h1>
        </header>
      </article>

      <p><b>Paper: </b><a href="#"><del>arXiv</del></a></p>
      <p><b>Authors: </b>Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron J. Weiss, Yonghui Wu.</p>

      <p><b>Abstract:</b>
        Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still a room for improvements in its efficiency during inference.
        This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder.
        This model, called <i>Parallel Tacotron</i>, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware.
        The use of the variational autoencoder helps to relax the one-to-many mapping nature of the text-to-speech problem.
        To further improve the naturalness, we introduce an iterative spectrogram loss, which is inspired by iterative refinement, and lightweight convolution, which can efficiently capture local contexts.
        Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective naturalness with significantly decreased inference time.
      </p>

      <p><a href="../../index.html">Click here for more from the Tacotron team.</a></p>

      <h2>Section Header</h2>

      <p><i>Section description.</i></p>

    </div>
  </body>
</html>
